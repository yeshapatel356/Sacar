{"cells": [{"metadata": {}, "cell_type": "code", "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='2a3c97b4-9a50-4a49-acbf-460288594b85', project_access_token='p-2+30RA4NO9tZEN6d5sgJn6LA==;M4ELCcLBQ29rCEf/jzU6fw==:ag7Uq7E48Qr4HjUgBPXlbEqgTJ/uaWGwFaA5/IRZf+vzgJqge1dj+Xov7ZDv+B2Y4ogdwH77Nw1FBgTq/om2NQ0vGs/Co7ZeRw==')\npc = project.project_context\n\nfrom ibm_watson_studio_lib import access_project_or_space\nwslib = access_project_or_space({'token':'p-2+30RA4NO9tZEN6d5sgJn6LA==;M4ELCcLBQ29rCEf/jzU6fw==:ag7Uq7E48Qr4HjUgBPXlbEqgTJ/uaWGwFaA5/IRZf+vzgJqge1dj+Xov7ZDv+B2Y4ogdwH77Nw1FBgTq/om2NQ0vGs/Co7ZeRw=='})", "execution_count": 43, "outputs": []}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "# Install and Import the NLTK library\n!pip install nltk\nimport nltk\n# Download the Punkt tokenizer from the NLTK repository\nnltk.download('punkt')\n# Download the stopwords corpus from the NLTK repository\nnltk.download('stopwords')", "execution_count": 44, "outputs": [{"output_type": "stream", "text": "Requirement already satisfied: nltk in /opt/conda/envs/Python-RT23.1-Premium/lib/python3.10/site-packages (3.8.1)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/envs/Python-RT23.1-Premium/lib/python3.10/site-packages (from nltk) (2023.3.23)\nRequirement already satisfied: tqdm in /opt/conda/envs/Python-RT23.1-Premium/lib/python3.10/site-packages (from nltk) (4.65.0)\nRequirement already satisfied: click in /opt/conda/envs/Python-RT23.1-Premium/lib/python3.10/site-packages (from nltk) (8.0.4)\nRequirement already satisfied: joblib in /opt/conda/envs/Python-RT23.1-Premium/lib/python3.10/site-packages (from nltk) (1.1.1)\n", "name": "stdout"}, {"output_type": "stream", "text": "[nltk_data] Downloading package punkt to /home/wsuser/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /home/wsuser/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n", "name": "stderr"}, {"output_type": "execute_result", "execution_count": 44, "data": {"text/plain": "True"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\ncos_client = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='rX5ySY4M7bEzYU0eSIrFIxwJOaknsea8I2m0oFm8uWeP',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url='https://s3.private.us-south.cloud-object-storage.appdomain.cloud')\n\nbucket = 'sacarcarrentingapplication-donotdelete-pr-flygcd2aonfzrf'\nobject_key = 'train.csv'\n\nbody = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_train = pd.read_csv(body)\n\n\nobject_key = 'test.csv'\n\nbody = cos_client.get_object(Bucket=bucket,Key=object_key)['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_test = pd.read_csv(body)", "execution_count": 45, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(df_train.shape, df_test.shape)", "execution_count": 46, "outputs": [{"output_type": "stream", "text": "(100, 11) (50, 11)\n", "name": "stdout"}]}, {"metadata": {"collapsed": true, "id": "b7bc6284-9fce-4089-bdb9-80988897cfa6"}, "cell_type": "markdown", "source": "# Analyzing Car Complaints with Watson NLP\n\nThis notebook demonstrates how to analyze car complaints using Watson NLP.\n\nThe data that is used in this notebook is taken from the National Highway and Transit Association (NHTSA) of the US Department of Transportation. The NHTSA records complaints from car owners, and makes them available regularly.  \n\n## What you'll learn in this notebook\n\nWatson NLP offers _blocks_ for various NLP tasks. This notebooks shows:\n\n- **Syntax analysis** with _Syntax block_ for English (`syntax_izumo_en_stock`). This block extracts nouns and their _lemma_ (base form) from the car complaints. This way, for the nouns _wheels_ and _wheel_, the block collects _wheel_. The most frequently used noun lemmas are typical problems that review authors talk about.\n- **Noun phrase extraction** with _Noun Phrases block_ for English (`noun-phrases_rbr_en_stock`). This block extracts larger noun phrases from car complaints to yield additional insights. \n- **How to correlate structured data with the analysis results**, with _association rule mining_ . This is used to find the most significant nouns for a particular car model. \n\n\n## Table of Contents\n\n1. [Load the car complaints](#loadData)\n1. [Analyze complaint descriptions with Watson NLP](#analyzeComplaints)\n1. [Explore and analyze the results](#exploreResults)\n1. [Identify nouns and noun phrases that are _specific_ to a given Model with Association Rule Mining](#correlate_n_nph)\n1. [Summary](#summary)"}, {"metadata": {"id": "a5173472-33a9-453a-8f6d-0a2cf6753151"}, "cell_type": "markdown", "source": "Begin by importing the Watson NLP library that is used throughout the notebook."}, {"metadata": {"id": "fe649f6d-eb92-443d-8166-8bbeb346c3a6"}, "cell_type": "code", "source": "import watson_nlp", "execution_count": 47, "outputs": []}, {"metadata": {"id": "9f8c6960-db90-466c-9f39-3747d3ddfc89"}, "cell_type": "markdown", "source": "<a id=\"loadData\"></a>\n## Load the car complaints"}, {"metadata": {"id": "6b4bad57-511a-45b2-b6e6-929cb2a328f0"}, "cell_type": "markdown", "source": "The data set used in this notebook from the National Highway and Transit Association (NHTSA) contains complaint records from car owners. The data sample provides information about the make and model of the car, the trouble-causing component, and a brief description of the issue. It also records the company's response to the issue described by the customer, as well as the customer's overall satisfaction level. The data set contains a total of 32 columns and 558 rows.\n\nLoad the car complaints into a dataframe."}, {"metadata": {}, "cell_type": "code", "source": "# Split the data into training and test sets, keeping only the Customer_Service and Satisfaction columns\ndf_train = df_train[['Customer_Service', 'Satisfaction']]\ndf_test = df_test[['Customer_Service', 'Satisfaction']]", "execution_count": 48, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Embed the text columns\n\nCheck if there is any watson embedding technique available"}, {"metadata": {}, "cell_type": "code", "source": "# Split the data into training and test sets.\ntrain_X, train_y = df_train['Customer_Service'], df_train['Satisfaction']\ntest_X, test_y = df_test['Customer_Service'], df_test['Satisfaction']", "execution_count": 49, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Get the shape of the 'Satisfaction' column in the training dataframe.\ndf_train['Satisfaction'].shape", "execution_count": 50, "outputs": [{"output_type": "execute_result", "execution_count": 50, "data": {"text/plain": "(100,)"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "train_X.shape, train_y.shape", "execution_count": 51, "outputs": [{"output_type": "execute_result", "execution_count": 51, "data": {"text/plain": "((100,), (100,))"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "# Embed the data\nimport gensim\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nimport pandas as pd", "execution_count": 52, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "\ndef preprocess(text: str) -> str:\n    # Remove punctuations, numbers and stop words\n    text = re.sub(r'[^\\w\\s]', '', text)\n    number_pattern = re.compile(r'\\d+')\n    text = re.sub(number_pattern, '', text)\n    stp_wrds = stopwords.words('english')\n    text = ' '.join([word for word in text.split() if word not in stp_wrds])\n    #Converts to lowercase\n    return text.lower()", "execution_count": 53, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Preprocess the text in the train_X and test_X DataFrames\ntrain_X = train_X.apply(preprocess)\ntest_X = test_X.apply(preprocess)", "execution_count": 54, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from gensim.models import Word2Vec\nembedding_size = 128\n\nsentences = pd.concat([train_X, test_X])", "execution_count": 180, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model = Word2Vec(sentences, vector_size=embedding_size, window=2, min_count=2)", "execution_count": 181, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "def get_word2vec_embeddings(text, model):\n    text_embedding = np.zeros(embedding_size)\n    vectors = []\n    word_count = 0\n    for word in text:\n        # TODO: Maybe take padding or use GloVe\n        text_embedding = text_embedding + model.wv[word]\n        word_count += 1\n    text_embedding = text_embedding / word_count\n    return text_embedding", "execution_count": 182, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "train_X = train_X.apply(lambda item: get_word2vec_embeddings(item, model))\ntest_X = test_X.apply(lambda item: get_word2vec_embeddings(item, model))", "execution_count": 183, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "train_X = np.concatenate(train_X).reshape(100, 128)\ntest_X = np.concatenate(test_X).reshape(50, 128)", "execution_count": 184, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.metrics import f1_score, precision_score, recall_score\n\ndef get_metrics(pred, true) -> dict:\n    return {\n        'f1-score': f1_score(pred, true),\n        'precision': precision_score(pred, true),\n        'recall': recall_score(pred, true)\n    }", "execution_count": 200, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Build Random Forest and Naive Bayes Model\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=0)\nclf.fit(train_X, train_y)\n\npred_y = clf.predict(test_X)\n\nget_metrics(pred_y, test_y)", "execution_count": 201, "outputs": [{"output_type": "execute_result", "execution_count": 201, "data": {"text/plain": "{'f1-score': 0.742857142857143,\n 'precision': 0.8125,\n 'recall': 0.6842105263157895}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.naive_bayes import GaussianNB\ngnb = GaussianNB()\ngnb.fit(train_X, train_y)\n\npred_y = gnb.predict(test_X)\nget_metrics(pred_y, test_y)", "execution_count": 202, "outputs": [{"output_type": "execute_result", "execution_count": 202, "data": {"text/plain": "{'f1-score': 0.5384615384615384, 'precision': 0.4375, 'recall': 0.7}"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": 35, "outputs": [{"output_type": "execute_result", "execution_count": 35, "data": {"text/plain": "pandas.core.series.Series"}, "metadata": {}}]}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "e1d58153-e5d4-45c2-afbc-a326f257b81b"}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}, {"metadata": {"id": "8bf98058-432d-4828-b0de-19d45862d32e"}, "cell_type": "markdown", "source": "## Authors"}, {"metadata": {"id": "08196167-6410-4074-be74-d1875b110fe4"}, "cell_type": "markdown", "source": "__Alexander Lang__ IBM, Germany\n\n__Simone Zerfass__ IBM, Germany"}, {"metadata": {"id": "f420e93e-82e9-4dca-b3cf-3928f005ba2f"}, "cell_type": "markdown", "source": "# <hr>\nCopyright \u00a9 2021 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}